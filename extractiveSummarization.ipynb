{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of extractiveSummarization",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtfONfm3f4Qv"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from time import time\n",
        "from datetime import timedelta\n",
        "from os.path import join, exists\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from transformers import BertModel, RobertaModel\n",
        "\n",
        "# from utils import read_jsonl, get_data_path, get_result_path\n",
        "# from dataloader import MatchSumPipe\n",
        "# from metrics import MarginRankingLoss, ValidMetric, MatchRougeMetric\n",
        "# from callback import MyCallback\n",
        "\n",
        "from clsHelper import read_jsonl, get_data_path, get_result_path\n",
        "from clsHelper import MatchSumPipe\n",
        "from clsHelper import MarginRankingLoss, ValidMetric, MatchRougeMetric\n",
        "from clsHelper import MyCallback\n",
        "\n",
        "# from model import MatchSum\n",
        "from fastNLP.core.trainer import Trainer\n",
        "from fastNLP.core.tester import Tester\n",
        "from fastNLP.core.callback import SaveModelCallback\n",
        "\n",
        "class MatchSum(nn.Module):\n",
        "    \n",
        "    def __init__(self, candidate_num, encoder, hidden_size=768):\n",
        "        super(MatchSum, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.candidate_num  = candidate_num\n",
        "        \n",
        "        if encoder == 'bert':\n",
        "            self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        else:\n",
        "            self.encoder = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "    def forward(self, text_id, candidate_id, summary_id):\n",
        "        \n",
        "        batch_size = text_id.size(0)\n",
        "        \n",
        "        pad_id = 0     # for BERT\n",
        "        if text_id[0][0] == 0:\n",
        "            pad_id = 1 # for RoBERTa\n",
        "\n",
        "        # get document embedding\n",
        "        input_mask = ~(text_id == pad_id)\n",
        "        out = self.encoder(text_id, attention_mask=input_mask)[0] # last layer\n",
        "        doc_emb = out[:, 0, :]\n",
        "        assert doc_emb.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
        "        \n",
        "        # get summary embedding\n",
        "        input_mask = ~(summary_id == pad_id)\n",
        "        out = self.encoder(summary_id, attention_mask=input_mask)[0] # last layer\n",
        "        summary_emb = out[:, 0, :]\n",
        "        assert summary_emb.size() == (batch_size, self.hidden_size) # [batch_size, hidden_size]\n",
        "\n",
        "        # get summary score\n",
        "        summary_score = torch.cosine_similarity(summary_emb, doc_emb, dim=-1)\n",
        "\n",
        "        # get candidate embedding\n",
        "        candidate_num = candidate_id.size(1)\n",
        "        candidate_id = candidate_id.view(-1, candidate_id.size(-1))\n",
        "        input_mask = ~(candidate_id == pad_id)\n",
        "        out = self.encoder(candidate_id, attention_mask=input_mask)[0]\n",
        "        candidate_emb = out[:, 0, :].view(batch_size, candidate_num, self.hidden_size)  # [batch_size, candidate_num, hidden_size]\n",
        "        assert candidate_emb.size() == (batch_size, candidate_num, self.hidden_size)\n",
        "        \n",
        "        # get candidate score\n",
        "        doc_emb = doc_emb.unsqueeze(1).expand_as(candidate_emb)\n",
        "        score = torch.cosine_similarity(candidate_emb, doc_emb, dim=-1) # [batch_size, candidate_num]\n",
        "        assert score.size() == (batch_size, candidate_num)\n",
        "\n",
        "        return {'score': score, 'summary_score': summary_score}\n",
        "\n",
        "def configure_training(args):\n",
        "    devices = [int(gpu) for gpu in args.gpus.split(',')]\n",
        "    params = {}\n",
        "    params['encoder']       = args.encoder\n",
        "    params['candidate_num'] = args.candidate_num\n",
        "    params['batch_size']    = args.batch_size\n",
        "    params['accum_count']   = args.accum_count\n",
        "    params['max_lr']        = args.max_lr\n",
        "    params['margin']        = args.margin\n",
        "    params['warmup_steps']  = args.warmup_steps\n",
        "    params['n_epochs']      = args.n_epochs\n",
        "    params['valid_steps']   = args.valid_steps\n",
        "    return devices, params\n",
        "\n",
        "def train_model(args):\n",
        "    \n",
        "    # check if the data_path and save_path exists\n",
        "    data_paths = get_data_path(args.mode, args.encoder)\n",
        "    for name in data_paths:\n",
        "        assert exists(data_paths[name])\n",
        "    if not exists(args.save_path):\n",
        "        os.makedirs(args.save_path)\n",
        "    \n",
        "    # load summarization datasets\n",
        "    datasets = MatchSumPipe(args.candidate_num, args.encoder).process_from_file(data_paths)\n",
        "    print('Information of dataset is:')\n",
        "    print(datasets)\n",
        "    train_set = datasets.datasets['train']\n",
        "    valid_set = datasets.datasets['val']\n",
        "    \n",
        "    # configure training\n",
        "    devices, train_params = configure_training(args)\n",
        "    with open(join(args.save_path, 'params.json'), 'w') as f:\n",
        "        json.dump(train_params, f, indent=4)\n",
        "    print('Devices is:')\n",
        "    print(devices)\n",
        "\n",
        "    # configure model\n",
        "    model = MatchSum(args.candidate_num, args.encoder)\n",
        "    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0)\n",
        "    \n",
        "    callbacks = [MyCallback(args), \n",
        "                 SaveModelCallback(save_dir=args.save_path, top=5)]\n",
        "    \n",
        "    criterion = MarginRankingLoss(args.margin)\n",
        "    val_metric = [ValidMetric(save_path=args.save_path, data=read_jsonl(data_paths['val']))]\n",
        "    \n",
        "    assert args.batch_size % len(devices) == 0\n",
        "    \n",
        "    trainer = Trainer(train_data=train_set, model=model, optimizer=optimizer,\n",
        "                      loss=criterion, batch_size=args.batch_size,\n",
        "                      update_every=args.accum_count, n_epochs=args.n_epochs, \n",
        "                      print_every=10, dev_data=valid_set, metrics=val_metric, \n",
        "                      metric_key='ROUGE', validate_every=args.valid_steps, \n",
        "                      save_path=args.save_path, device=devices, callbacks=callbacks)\n",
        "    \n",
        "    print('Start training with the following hyper-parameters:')\n",
        "    print(train_params)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "def test_model(args):\n",
        "\n",
        "    models = os.listdir(args.save_path)\n",
        "    \n",
        "    # load dataset\n",
        "    data_paths = get_data_path(args.mode, args.encoder)\n",
        "    datasets = MatchSumPipe(args.candidate_num, args.encoder).process_from_file(data_paths)\n",
        "    print('Information of dataset is:')\n",
        "    print(datasets)\n",
        "    test_set = datasets.datasets['test']\n",
        "    \n",
        "    # need 1 gpu for testing\n",
        "    device = int(args.gpus)\n",
        "    \n",
        "    args.batch_size = 1\n",
        "\n",
        "    for cur_model in models:\n",
        "        \n",
        "        print('Current model is {}'.format(cur_model))\n",
        "\n",
        "        # load model\n",
        "        model = torch.load(join(args.save_path, cur_model))\n",
        "    \n",
        "        # configure testing\n",
        "        dec_path, ref_path = get_result_path(args.save_path, cur_model)\n",
        "        test_metric = MatchRougeMetric(data=read_jsonl(data_paths['test']), dec_path=dec_path, \n",
        "                                  ref_path=ref_path, n_total = len(test_set))\n",
        "        tester = Tester(data=test_set, model=model, metrics=[test_metric], \n",
        "                        batch_size=args.batch_size, device=device, use_tqdm=False)\n",
        "        tester.test()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='training/testing of MatchSum'\n",
        "    )\n",
        "    parser.add_argument('--mode', required=True,\n",
        "                        help='training or testing of MatchSum', type=str)\n",
        "\n",
        "    parser.add_argument('--save_path', required=True,\n",
        "                        help='root of the model', type=str)\n",
        "    # example for gpus input: '0,1,2,3'\n",
        "    parser.add_argument('--gpus', required=True,\n",
        "                        help='available gpus for training(separated by commas)', type=str)\n",
        "    parser.add_argument('--encoder', required=True,\n",
        "                        help='the encoder for matchsum (bert/roberta)', type=str)\n",
        "\n",
        "    parser.add_argument('--batch_size', default=5,\n",
        "                        help='the training batch size', type=int)\n",
        "    parser.add_argument('--accum_count', default=2,\n",
        "                        help='number of updates steps to accumulate before performing a backward/update pass', type=int)\n",
        "    parser.add_argument('--candidate_num', default=10,\n",
        "                        help='number of candidates summaries', type=int)\n",
        "    parser.add_argument('--max_lr', default=2e-5,\n",
        "                        help='max learning rate for warm up', type=float)\n",
        "    parser.add_argument('--margin', default=0.01,\n",
        "                        help='parameter for MarginRankingLoss', type=float)\n",
        "    parser.add_argument('--warmup_steps', default=100,\n",
        "                        help='warm up steps for training', type=int)\n",
        "    parser.add_argument('--n_epochs', default=1,\n",
        "                        help='total number of training epochs', type=int)\n",
        "    parser.add_argument('--valid_steps', default=100,\n",
        "                        help='number of update steps for validation and saving checkpoint', type=int)\n",
        "\n",
        "    args = parser.parse_known_args()[0]\n",
        "    \n",
        "    if args.mode == 'train':\n",
        "        print('Training process of MatchSum !!!')\n",
        "        train_model(args)\n",
        "    else:\n",
        "        print('Testing process of MatchSum !!!')\n",
        "        test_model(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}